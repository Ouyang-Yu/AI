AI写诗？？ AI创作小说？？ 近年来人们时常听到这类新闻，听上去很不可思议，那么今天我们来一探究竟，这种功能是如何通过深度学习来实现的。通常文本生成的基本策略是借助语言模型，这是一种基于概率的模型，可根据输入数据预测下一个最有可能出现的词，而文本作为一种序列数据 (sequence data)，词与词之间存在上下文关系，所以使用循环神经网络 (RNN) 基本上是标配，这样的模型被称为神经语言模型 (neural language model)。在训练完一个语言模型后，可以输入一段初始文本，让模型生成一个词，把这个词加入到输入文本中，再预测下一个词。这样不断循环就可以生成任意长度的文本了，如下图给定一个句子 ”The cat sat on the m“ 可生成下一个字母上图中语言模型 (language model) 的预测输出其实是字典中所有词的概率分布，而通常会选择生成其中概率最大的那个词。不过图中出现了一个采样策略 (sampling strategy)，这意味着有时候我们可能并不想总是生成概率最大的那个词。设想一个人的行为如果总是严格遵守规律缺乏变化，容易让人觉得乏味；同样一个语言模型若总是按概率最大的生成词，那么就容易变成 XX讲话稿了。因此在生成词的过程中引入了采样策略，在最后从概率分布中选择词的过程中引入一定的随机性，这样一些本来不大可能组合在一起的词可能也会被生成，进而生成的文本有时候会变得有趣甚至富有创造性。采样的关键是引入一个temperature参数，用于控制随机性。假设 p(x) 为模型输出的原始分布，则加入 temperature 后的新分布为训练了100个epoch后，可以开始生成文本了，主要有以下几个步骤：将已生成的文本以同样的方式 one-hot 编码，用训练好的模型得出所有字符的概率分布。根据给定的 temperature 得到新的概率分布。从新的概率分布中抽样得到下一个字符。将生成的新字符加到最后，并去掉原文本的第一个字符。接下来我们来看论文中方法的具体实现，用的库是 Keras。论文作者 Gatys 等人没有从头训练一个CNN，而是使用了预训练 (pre-trained) 的 VGG-19 网络来对图片进行特征提取，这些特征可以帮助我们去衡量两个图像的内容差异和风格差异。大致流程如下：将参考图片，原始图片，生成图片同时输入 VGG-19 网络，计算各个隐藏层的输出值。定义并计算上文中描述的损失函数。使用优化方法最小化损失函数。全书分为两大部分，第一部分是对于深度学习的全局介绍，包括其与人工智能、机器学习的关系，一些相关的基本概念如张量(tensor)、梯度下降、神经网络、反向传播算法等等。其中第三章举了三个简单的例子，分别对应的任务是二分类、多分类和回归，让读者快速了解 Keras 的基本使用方法，熟悉使用深度学习处理数据问题的典型流程。这里插一句，对于入门这个领域一直存在着两种声音： 一种是top-down式，从实践入手，讲究先沉浸在实际的项目中，再回到理论；另一种是bottom-up式，典型的学习方式是先学习各类书和课程，从理论和数学入手，再到实践。本书作者 François Chollet 是坚定的 top-down 派，记得之前网上看过一篇文章就是讲作者和其他业界大佬撕逼哪种方式好。。。 作者在一个访谈中也提到：模型是事先定义好的神经网络架构，深度学习的模型中一般有着上百万个的权重(weights)，这些权重决定了输入数据X后模型会输出什么样的预测结果Y‘，而所谓的“学习”就是寻找合适的权重使得预测结果和真实目标尽可能接近。而说道接近就涉及到了如何度量两个值的接近程度，这就是策略要做的事情，其实就是定义合适的目标函数(损失函数)。目标函数以真实目标Y和预测结果Y'作为输入，输出一个损失值(loss score)作为反馈信号来更新权重以减少这个损失值，而具体实现这一步骤的就是算法，即上图中的优化器(optimizer)，优化器的典型例子就是梯度下降以及其各种变种。
所以这张图清晰地描绘了神经网络整个的训练过程，开始时权重被初始化为一些随机值，所以其预测结果和真实目标Y相差较大，进而损失值也会很大。随着优化器不断地更新权重，使得损失值也会越来越小，最后当损失值不再减少时我们就得到了一个训练好的神经网络。Keras的设计基本上也是按照这个思路，先定义整个网络，具体表现为加各种各样的层(layer)，再指定相应的损失函数和优化器，就可以开始训练了。在另外一页作者说可以把层(layers)想象成深度学习的乐高积木，那么是时候祭出这张图了：第三章提供了三个简单的例子以让读者快速上手Keras，分别为IMDB电影数据集的情感分析、新闻主题分类和房价预测，这三个例子对应了二分类、多分类和回归这三个机器学习中最常遇到的问题。虽然都是比较简单的例子，但麻雀虽小却也五脏俱全，书中依然对各种可能碰到的问题做了比较详尽的阐述，比如如何将数据预处理成适合神经网络的输入，如何定义神经网络层，如何标准化数据，如何处理过拟合和欠拟合等等。而在这之前，作者简要介绍了Keras。Keras是一个基于Python的深度学习框架，最大的特点是易用性，近年来增长势头迅猛，在Kaggle上也非常流行，我印象中几个比赛前几名的Solution如果是神经网络的话基本都用了Keras。在机器学习竞赛中，这些获胜者很少是从一开始就想出了最好的想法，然后简单地部署它，提交结果，最后很快忘了这回事。开发一个好的模型需要对最初的想法进行多次迭代，直到时间到了；你总是能进一步改进你的模型。最终的模型与你最初尝试解决这个问题时所设想的方案几乎没有任何共同点，因为一个事先想好的方案基本上从来不会在现实试验中存活下来。所以赢不是说你最初的理论构想有多棒，而在于你的设想有多少能通过现实的考验。你不会输给比你更聪明的人，你会输给那些比你重复更多实验的人，他们一点点精化他们的模型。如果你把 Kaggle 上的团队按照实验的次数排名，我保证你会发现试验次数排名与最后的竞争排行榜有很强的关联。Keras 被设计作为一种可快速构建许多不同模型的原型的方法，其关注的重点是尽可能减少从想法到实验结果验证之间所需的时间。Keras API 和工作流程基本上可以减少设置新实验的开销（overhead）（代码开销和认知开销）。所以使用 Keras 让你可以更快地迭代，尝试更多事物。最终，这能让你赢得比赛（或发表论文）。能够以最小的延迟将想法变成结果对实现好的研究来说是很关键的——这是 Keras 背后的核心信念之一。























